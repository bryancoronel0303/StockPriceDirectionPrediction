# -*- coding: utf-8 -*-
"""logistic_pso

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rm1CAeE1Jf7phPo0_mkQNXl1yth8VPaN

# 0. Install/Load Dependencies
"""

!pip install pyswarms

import numpy as np
import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
import seaborn as sns
import matplotlib.pyplot as plt
#import pyswarms as ps
from sklearn.preprocessing import StandardScaler

"""# 1. Data Pre-Processing, Feature Eng"""

df = pd.read_csv("/content/drive/MyDrive/spy.csv")

df['Average'] = df[['Open', 'Close', 'High', 'Low']].mean(axis=1)
df['HL_PCT'] = (df['High'] - df['Low']) / df['Low']
df['PCT_change'] = (df['Close'] - df['Open']) / df['Open']

df['Volume_pct_change'] = df['Volume'].pct_change() 
df["Target"] = (df["Close"] > df["Open"]).shift(periods=-1, fill_value=0).astype(int)

def macd(data, short=12, long=26, signal=9):
    exp1 = data['Close'].ewm(span=short, adjust=False).mean()
    exp2 = data['Close'].ewm(span=long, adjust=False).mean()
    macd = exp1 - exp2
    signal_line = macd.ewm(span=signal, adjust=False).mean()
    return macd, signal_line
def rsi(data, periods=14):
    delta = data['Close'].diff()
    gain, loss = delta.copy(), delta.copy()
    gain[gain < 0] = 0
    loss[loss > 0] = 0

    avg_gain = gain.rolling(window=periods).mean()
    avg_loss = -loss.rolling(window=periods).mean()

    rs = avg_gain / avg_loss
    rsi = 100 - (100 / (1 + rs))
    return rsi
df['RSI'] = rsi(df)
df['MACD'], df['Signal'] = macd(df)

df = df.drop(['Date','Close', 'Volume', 'Year', 'Week'], axis=1).assign(Target=df.pop('Target'))
df.dropna(inplace=True)


X = df.drop(['Target'],axis=1).values
y = df['Target'].values

df

"""# 2. EDA"""

sns.pairplot(df, hue='Target');

# Calculate correlation matrix
corr_matrix = df.corr()
fig, ax = plt.subplots(figsize=(10, 6))
# Plot correlation matrix as a heatmap
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix of Features')
plt.show()

"""# 3. Implement Model"""

# Create an instance of StandardScaler
scaler = StandardScaler()
# Fit and transform the entire dataset
X_scaled = scaler.fit_transform(X)

classifier = LogisticRegression(C = 0.28438387) # from basin-hopping
# Define objective function
def f_per_particle(m, alpha):
    """Computes for the objective function per particle
    Inputs
    ------
    m : numpy.ndarray
        Binary mask that can be obtained from BinaryPSO, will
        be used to mask features.
    alpha: float (default is 0.5)
        Constant weight for trading-off classifier performance
        and number of features
    Returns
    -------
    numpy.ndarray
        Computed objective function
    """
    total_features = 13
    # Get the subset of the features from the binary mask
    if np.count_nonzero(m) == 0:
        X_subset = X_scaled
    else:
        X_subset = X_scaled[:,m==1]
    # Perform classification and store performance in P
    classifier.fit(X_subset, y)
    P = (classifier.predict(X_subset) == y).mean()
    # Compute for the objective function
    j = (alpha * (1.0 - P)
        + (1.0 - alpha) * (1 - (X_subset.shape[1] / total_features)))
    return j
def f(x, alpha=0.88):
    """Higher-level method to do classification in the
    whole swarm.

    Inputs
    ------
    x: numpy.ndarray of shape (n_particles, dimensions)
        The swarm that will perform the search
    Returns
    -------
    numpy.ndarray of shape (n_particles, )
        The computed loss for each particle
    """
    n_particles = x.shape[0]
    j = [f_per_particle(x[i], alpha) for i in range(n_particles)]
    return np.array(j)

# Initialize swarm, arbitrary
options = {'c1': 0.5, 'c2': 0.5, 'w':0.9, 'k': 30, 'p':2}
# Call instance of PSO
dimensions = 13 # dimensions should be the number of features
optimizer = ps.discrete.BinaryPSO(n_particles=30, dimensions=dimensions, options=options)
# Perform optimization
cost, pos = optimizer.optimize(f, iters=500, verbose=2)

"""# 4. Evaluate

{'solver': 'saga',
 'penalty': 'l1',
 'max_iter': 789,
 'fit_intercept': True,
 'C': 0.008697490026177835}
"""

# Create two instances of LogisticRegression
classfier = LogisticRegression(C = 0.28438387)
# Get the selected features from the final positions
X_selected_features = X[:,pos==1]  # subset
# Perform classification and store performance in P
classifier.fit(X_selected_features, y)
# Compute performance
subset_performance = (classifier.predict(X_selected_features) == y).mean()
print('Subset performance: %.3f' % (subset_performance))